{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snowball_spider import *\n",
    "import pytz\n",
    "import psycopg2\n",
    "import string\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def grab_comments(stock_code, count):\n",
    "    ''' returns stock object in dictionary form: stock_object -> {'stock', 'data'} where data -> multiple comment objects of dictionary type '''\n",
    "    stock_object = {}\n",
    "    snowball = Snowball(stock_code)\n",
    "    page_list = snowball.get_all_pages(count)  # first page first post: page_list[0][0]; type: dict\n",
    "\n",
    "    comments = snowball.find_post_objects(page_list)[0]  # all raw comment json files\n",
    "    print('\\nNumber of comments for stock {} in current page(s):'.format(stock_code), len(comments),\n",
    "          end='\\n\\n')  # number of comments on all requested pages\n",
    "    print('-' * 115)\n",
    "    # parsed_comments = snowball.parse_comment_text(comments)#all parsed comment texts only\n",
    "\n",
    "    comment_object_list = snowball.comment_object_list(comments)\n",
    "    # for comment_object in comment_object_list:\n",
    "    #     print(type(comment_object))\n",
    "    #     print(comment_object)\n",
    "    #     print('-'*115)\n",
    "    stock_object['stock'] = stock_code\n",
    "    stock_object['data'] = comment_object_list \n",
    "    return stock_object\n",
    "\n",
    "def look_up_db(db, cur, table_name, option):\n",
    "# look up database:\n",
    "    printable = ['by_count', 'by_period', 'find_count']\n",
    "    cases = {\n",
    "        'find_count': lambda: db.find_record_number(table_name),\n",
    "        'by_count': lambda: db.select_comments_by_count(table_name, 5),\n",
    "        'by_period': lambda: db.select_comments_by_period(table_name, ['10 hour',]),\n",
    "        'find_last_time': lambda: db.find_time_last_comment(table_name)\n",
    "    }\n",
    "    cases[option]()\n",
    "    db_temp = cur.fetchall()\n",
    "    if option in printable:\n",
    "        for data in db_temp:\n",
    "            print(data)\n",
    "            print('-' * 115)\n",
    "    return db_temp\n",
    "\n",
    "'''\n",
    "TO DO: create function that batch classify sentiment\n",
    "'''\n",
    "\n",
    "def grab_rdd(stock_code):\n",
    "    return grab_comments(stock_code, 1)\n",
    "\n",
    "def main():\n",
    "    stock_code_list =['00700', 'BABA', 'AAPL'] # set code list to grab comments\n",
    "    stock_list = [] # a list that contains all stock comment objects\n",
    "    # Connect to database:\n",
    "    username, dbname = 'jian', 'postgres'\n",
    "    try:\n",
    "        conn = psycopg2.connect(user=username, dbname=dbname)\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        db = DB_Operations(cur)\n",
    "        # db.drop_table()\n",
    "        for t in stock_code_list:\n",
    "            db.initialize_database(table_name='stock_'+t)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "    client = AipNlp_API('AipNlp.txt').connect_to_AipNlp()\n",
    "#     print(client.sentimentClassify(text)['items'])\n",
    "\n",
    "    # spark contains whole steps:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "#     for stock in stock_code_list:\n",
    "#         stock_list.append(grab_comments(stock, 1)) # One stock object, # of pages\n",
    "\n",
    "    stock_list = sc.parallelize(stock_code_list).map(grab_rdd).collect()\n",
    "#     stock_list.filter(lambda x: x[\"stock\"]=='00700').collect()\n",
    "    sc.stop()\n",
    "    \n",
    "    print(stock_list[0])\n",
    "\n",
    "    for stock in stock_list:\n",
    "        db.batch_insert_comments('stock_'+stock['stock'].lower(), stock['data'], sentiment_list)\n",
    "\n",
    "\n",
    "    last_comment_time_web = stock_list[0]['data'][0]['timestamp']\n",
    "    last_comment_time_db = look_up_db(db, cur, 'stock_'+stock_list[0]['stock'], 'find_last_time')[0][0] # timestamp format, to compare with time of latest comment on website\n",
    "    print('The time of last comment for stock 00700 up to date:', datetime.fromtimestamp(last_comment_time_web/1000.0).astimezone(pytz.timezone('Asia/Shanghai')).strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "    print('The time of last comment for stock 00700 recorded at:', datetime.fromtimestamp(last_comment_time_db/1000.0).astimezone(pytz.timezone('Asia/Shanghai')).strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "    print('-'*115)\n",
    "    look_up_db(db, cur, 'stock_'+stock_code_list[0].lower(), 'by_count')\n",
    "\n",
    "\n",
    "    if conn:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
